{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/soai2.jpg\" alt=\"School of ai Algiers\" title=\"School of ai Algiers\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Dans ce jupyter notebook, on traitera les bases des outils de primordiaux pour débuter en machine learning, à noter : numpy, pandas, matplotlib. On traitera principalement du data cleaning ainsi que du data visualization.\n",
    "Ce document a été réalisé dans le cadre du 1er workshop de School of ai Algiers.<br>\n",
    "<img src=\"img/method.png\"/>\n",
    "\n",
    "# Les deux voies\n",
    "## Thérorie\n",
    "<ul>\n",
    "    <li>Algorithmes from Scratch</li>\n",
    "    <li>\n",
    "        Maths Heavy<br>\n",
    "        Probabilités\n",
    "        <img src=\"img/likelihood.png\"/>\n",
    "        Analyse\n",
    "        <img src=\"img/gd.png\"/>\n",
    "        Algèbre\n",
    "        <img src=\"img/la.png\"/>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "## Pratique\n",
    "<ul>\n",
    "    <li>\n",
    "        Utilisation de bibliothèques prêtes à l'utilisation (Sk learn, tensorflow, Pytorch, Keras)<br>\n",
    "        <img src=\"img/code.png\"/>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparez vos outils\n",
    "\n",
    "## Python\n",
    "### Pourquoi python\n",
    "<ul>\n",
    "    <li>Syntaxe simple et très claire</li>\n",
    "    <li>Richesse en librairies ML</li>\n",
    "    <li>Taille de la communauté</li>\n",
    "</ul>\n",
    "\n",
    "## Librairies nécessaires\n",
    "<ul>\n",
    "    <li>Numpy</li>\n",
    "    <li>Pandas</li>\n",
    "    <li>Matplotlib</li>\n",
    "    <li>Sci-kit Learn</li>\n",
    "</ul>\n",
    "\n",
    "## numpy\n",
    "### Qu'est ce que numpy ?\n",
    "    Librairie python permettant de faciliter la manipulation des tableaux (vecteurs/matrices)\n",
    "### Pourquoi pas de simples tableaux\n",
    "    - Calculs plus rapides\n",
    "    - Manipulation plus simple\n",
    "    - Opérations prêtes à être utilisées (sur des vecteurs et des matrices)\n",
    "### Comment l'installer ?\n",
    "> <font color=red>**pip** install numpy</font>\n",
    "\n",
    "### Commencer à travailler avec numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Directement dans le code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npArray=np.array([1,2,3,4,5])\n",
    "npArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vecteur initialisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectNul=np.zeros(4)\n",
    "vectNul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### En créant une série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Générer la suite de nombres entre 1(inclus) et 10(exclus) par pas de 2\n",
    "serie=np.arange(1,10,2)\n",
    "print(serie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Depuis un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Données sous forme de matrice texte\n",
    "fromTxt = np.loadtxt('data.txt', usecols=range(4))\n",
    "print(fromTxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accéder aux éléments (Slicing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie= np.arange(1,10,2)\n",
    "serie[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie[2]=12\n",
    "serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=serie[1:4]\n",
    "print(\"Valeur de y : {}\".format(y))\n",
    "print(\"Valeur de serie: {}\".format(serie))\n",
    "y[0]=888\n",
    "serie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le slicing génère une vue sur le tableau (quand on change la variable, l'originale change aussi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter des éléments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout à la fin\n",
    "serie=np.arange(1,10,2)\n",
    "serie=np.append(serie,11)\n",
    "print(\"11 Ajouté : \",serie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forme d'un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changer la forme d'un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.reshape(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension d'un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taille ( nombre total d'élements )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Générer une matrice de zéros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Générer une matrice identité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity=np.eye(3)\n",
    "identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Générer une matrice de nombres aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mRand=np.random.rand(2,3)\n",
    "mRand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiplication matricielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut que le nombre de colonnes de la 1e matrice soit égale au nombre de lignes de la 2e  \n",
    "$ 1^e -----> (m,k) | (k,n)  <------ 2^e $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,2,3],\n",
    "            [1,1,1],\n",
    "            [0,0,1]])\n",
    "np.dot(A,identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transposée d'une matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrice inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ainv=np.linalg.inv(A)\n",
    "Ainv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques autres opérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=A/2\n",
    "print(\"Matrice objectif\\n\",A)\n",
    "print()\n",
    "print(\"Minimum\",np.amin(A))\n",
    "print(\"Maximum\",np.amax(A))\n",
    "print(\"Moyenne\",np.mean(A))\n",
    "print(\"Ecart type\",np.std(A))\n",
    "print(\"Partie entière\\n\",np.floor(A))\n",
    "print(\"Entier au dessus\\n\",np.ceil(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "Manipuler tous les éléments du tableau en une seule opération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.arange(0,10).reshape(5,2)\n",
    "newArr=arr-3\n",
    "print(arr)\n",
    "newArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([0,1,3])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retourner un masque indiquant quels éléments satisfont la condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau=np.arange(10)\n",
    "print(tableau)\n",
    "(tableau>5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retourner les indices qui satisfont la condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retourner les éléments qui satisfont la condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau[tableau!=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeurs statistiques\n",
    "### La moyenne\n",
    "\n",
    "### L'écart type\n",
    "C'est une valeur statistique permettant de caractériser \"l'éloignement\" ou la dispersion des données de leur moyenne\n",
    "\n",
    "### La médiane\n",
    "Représente la valeur centrale quand on ordonne les valeurs de notre échantillon.\n",
    "\n",
    "### Le mode\n",
    "Représente la valeur qui apparait le plus dans notre échantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas\n",
    "    Librairie python permettant la manipulation et l'analyse de données\n",
    "### Exemple\n",
    "Cet exemple représente un ensemble d'athlètes dont on dispose de certaines information :\n",
    "- <b>Name</b> : Prénom de l'athlète\n",
    "- <b>Wilaya</b> : Wilaya de naissance de l'athlète\n",
    "- <b>Age</b> : âge de l'athlète en années\n",
    "- <b>Taille</b> : Taille de l'athlète en mètres\n",
    "- <b>Poids</b> : Poids de l'athlète en kilogrammes\n",
    "- <b>Sexe</b> : Sexe de l'athlète\n",
    "- <b>Score</b> : le nombre de points remportés en compétition par l'athlète\n",
    "- <b>Address</b> : Adresse de l'athlète\n",
    "\n",
    "\n",
    "### DataFrame\n",
    "Un DataFrame est une structure de données dans pandas qui vous permettra de structurer vos données sous forme tabulaire.\n",
    "On trouve <br>\n",
    "en colonnes : les attributs<br>\n",
    "en lignes   : les observations\n",
    "\n",
    "<img src=\"img/df.png\" alt=\"Image d'un dataframe\" title=\"Dataframe\">\n",
    "\n",
    "\n",
    "### Commencer à travailler avec pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer des données\n",
    "#### Depuis un fichier csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "df=pd.read_csv(\"data.csv\",delimiter=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques valeurs statistiques sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélectionner aléatoirement n lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir une colonne en un autre type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].astype('float')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé sur les valeurs \"Catégorie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Décompte par catégorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wilaya'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Je veux classer les personnes par score et prendre les 5 meilleurs (Tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.sort_values(by='Score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Je veux la liste des personnes d'Alger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Wilaya\"]==\"Alger\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une compétion aux personnes de plus de 20 ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Age\"]>20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques valeurs statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Moyenne : \",df[\"Poids\"].mean())\n",
    "print(\"Maximum :\",df[\"Poids\"].max())\n",
    "print(\"Ecart type :\",df[\"Poids\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une année est passée je veux actualiser les âges (Appliquer une fonction à une colonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment(number):\n",
    "    return number+1\n",
    "#axis=1 pour ligne\n",
    "df['Age']=df['Age'].apply(increment)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sexe'] = df['Sexe'].replace({\"H\":\"Homme\",\"Male\":\"Homme\",\"home\":\"Homme\"})\n",
    "df['Wilaya'] = df['Wilaya'].replace({\"Algiers\":\"Alger\",\"Alg\":\"Alger\"})\n",
    "df['Taille'] = df[\"Taille\"].str.replace(\",\",\".\").astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Je veux codifier des catégories (Remplacer certaines valeurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sexe'] = df['Sexe'].map({\"Homme\":0,\"Femme\":1}) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Je veux étudier chaque catégorie séparément (Regrouper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"Sexe\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculs sur la base des colonnes et modification d'un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date de naissance'] = 2018- df[\"Age\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Je veux calculer l'IMC ! IMC = Poids / Taille²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IMC']=df['Poids']/df['Taille']**2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J'ai les mêmes données mais de deux sources différentes (Concaténer deux dataframes en ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df,df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J'ai une nouvelle donnée sur mes utilisateurs ! (Ajouter une colonne à un dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hSommeil=pd.DataFrame({\"Heures de sommeil\":[7,8,6,9,5,9]})\n",
    "hSommeil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df,hSommeil],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoir le dataframe en tant que tableau numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib\n",
    "    Librairie python permettant de faire des représentations graphiques de nos données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commencer à travailler avec matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#x=np.arange(-10,10,1)\n",
    "#y=x**2\n",
    "c=df.sort_values(by=\"Taille\",ascending=False)\n",
    "\n",
    "\n",
    "plt.plot(c[\"Taille\"],c[\"Score\"],label=\"Score selon la taille\",color=\"#ffe0a3\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Taille\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Courbe score selon la Taille\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagramme en bâtons \n",
    "### Selon le Sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"Homme\",\"Femme\"],df[\"Sexe\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selon la Wilaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df[\"Wilaya\"].unique(),df[\"Wilaya\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=np.random.normal(60,10,20000000)\n",
    "lol=np.random.normal(40,10,2000000)\n",
    "\n",
    "bins=np.arange(0,100,5)\n",
    "plt.hist([dist,lol],bins,density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuage de points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des données\n",
    "NOMBRE_POINTS=200\n",
    "x1=np.random.normal(4,0.5,NOMBRE_POINTS)\n",
    "y1=np.random.normal(3,0.4,NOMBRE_POINTS)\n",
    "x2=np.random.normal(3,0.5,NOMBRE_POINTS)\n",
    "y2=np.random.normal(2,0.3,NOMBRE_POINTS)\n",
    "\n",
    "plt.scatter(x1,y1,label=\"catégorie 1\",color=\"#13eaff\",marker=\"x\")\n",
    "plt.scatter(x2,y2,label=\"catégorie 2\",color=\"#ffee1f\")\n",
    "\n",
    "plt.scatter([4,3],[3,2],label=\"Centres\",color=\"r\",marker=\"*\",s=100)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"Taille\"],df[\"Poids\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use(\"Solarize_Light2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D'autres fonctionnalités \n",
    ">Graphes en temps réel <br>\n",
    "Graphes 3D <br>\n",
    "Sous graphes <br>\n",
    "\n",
    "<img src=\"img/subplot.png\" title=\"Image Sous graphe\" alt=\"Image d'une figure avec plusieurs sous graphes\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprendre plus\n",
    "matplotlib sentdex : https://www.youtube.com/playlist?list=PLQVvvaa0QuDfefDfXb9Yf0la1fPDKluPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "<img src=\"img/emotions.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le problème\n",
    "J'ai une phrase et je veux la classifier comme étant : \n",
    "- Positive : Happy\n",
    "- Négative : Sad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i think about trayvonmartin i can not help but worry about my own son!  i am so sad and angry right now! have been all day actually.',\n",
       " 'ilikepeoplewho makes me laugh when i am sad.',\n",
       " 'sometimes i just instantly get sad when i realize how blessed i am and i think about homeless people sleeping outside and hungry :(',\n",
       " 'i have zero pics from last night very sad :( but hey i am proud of that hat!!',\n",
       " 'i hear ya.  i tried for a while and am now watching goldmember. so sad',\n",
       " \"i feel happy, i feel sad. and i don't know who i am.,it’s not the way you plan it is how you make it happend.\",\n",
       " 'it was so good! but so sad. i cried like a baby!! i am so sad that its ending :(',\n",
       " 'no i am not in nyc these days... i know i am sad as well',\n",
       " 'why am i suddenly all sad?',\n",
       " \"follows everyone but everyone doesn't follow back now i am sad\",\n",
       " 'reading past convos i had with females, i am so oblivious and cannot take a hint. this is sad',\n",
       " 'i am sad that the chief of police has already made up his mind that the shooter was protecting himself.',\n",
       " 'i am on my pc!  the link is not working.  :: sad::',\n",
       " 'as mad and sad as i am about mike being killed, it was such a brilliant example of tragic writing at its best.',\n",
       " 'ohhhh come on ! now im sad. i am the one with really no friends at all.',\n",
       " 'oh my good!!!! too much feelings inside and any way to show them... how sad girl am i?? a big one',\n",
       " 'meh i am sad',\n",
       " 'recently found out blue mountain state is over . . . what am i suppose to watch now sad',\n",
       " 'seeing you at the same place i am at makes me very sad. sad that you didnt even say one fucking word to me.',\n",
       " 'i am actually kind of sad that its not flannel weather anymore... iloveflannel bestmaterialever afterwoolandleather',\n",
       " 'if any of your guys are followed by @diggy_simmons your rlly lucky cus im like trying and i need help cus i am going crazy. ugh i am sad :(',\n",
       " 'i am kinda sad. lol',\n",
       " 'whenever i am sad, angry, or bored, i eat.',\n",
       " 'good night all.  i am so sad that obama made us all slaves today.  maybe tomorrow will be a better day....peace to us all...',\n",
       " 'my sad had work this am smh happy bday tho uce!! i still owe u an sit',\n",
       " 'i want a dq blizzard. cheer me up i am sad :(',\n",
       " \"sad thing is my parents don't really know who i am.. they know me but dont know me.\",\n",
       " 'my fucking ear hurts, im sad asf, im mad asf. ughhh wtf am i gonna do with myself',\n",
       " \"i don't believe you..fuck ya i am i'm already sad cause i need some alcohol in my system\",\n",
       " \"i am so technology incapable, i cannot use tumbler xd it's so sad lol\",\n",
       " 'the only way im ever gonna get any flowers is if i buy them myself.....and thats sad. http://t.co/ltincx',\n",
       " 'i am sad like fucking shit',\n",
       " 'yeah i know :( sad day',\n",
       " 'i like tortured souls, sad as i am!!',\n",
       " 'well spring break is over in about  hour and i am sad:(',\n",
       " 'are you sad the season is ending? ? i am:(',\n",
       " 'just watched the season finally of jersey shore and i am sad its over :(',\n",
       " 'is it sad that i am super excited for the new season of frozen planet to start sunday on animal planet..',\n",
       " '(insert sad asshole tweet here) yea i wanna cuddle and am going to sleep alone right now... night bitches! dueces',\n",
       " 'dynastycrisis: http://t.co/zwsrhqm',\n",
       " 'when am i going to be able to see him without getting sad..',\n",
       " 'we r so sad! i am crying in the first  mins ;(',\n",
       " 'are you sad that the season is ending!? i know i am!',\n",
       " 'i am very sad to need to set an alarm for the morning. worst part of getting the boys to school.',\n",
       " 'kendra &amp; lexi .. who am i ? \"oh im crying again:( im so sad.. i really did like him:\\'( wahhh\" .... bitch moan bitch moan..',\n",
       " 'but seriously why am i so sad tonight?',\n",
       " 'when i am sad and mad i pick on my face till i rip flesh off. i need help -_-',\n",
       " \"as sad as i am that you're not part of my life anymore i'm at least glad someone else is still getting the chance to have you in theirs. ...\",\n",
       " \"me ?? i am so sad read it t.t sorry :'( rt @lovely_leejoon: @dewii_ you mean rani doesn't like you?i see..(t . t)\",\n",
       " \"i'm feeling so sad now... i will no longer be looking forward to thewalkingdead every sunday. what am i going to do?\",\n",
       " 'so sad i have  am class tomorrow wahwah',\n",
       " 'and if i am ever sad i feel like everyone thinks its because of you.',\n",
       " \"i don't remember what this episodes holds!! but you are all so sad, i am worried!! don'trememberyesterday\",\n",
       " \"i am.. its sad.  like i can't even kill a spidar.. my little cousins do it.. haah\",\n",
       " \"realize you fucking twat. i'm not dragging in a sad way. i am actually quite happy! and i wouldn't say i don't care for you.\",\n",
       " 'doing the only thing i know to do when i am sad and depressed, eating cheese',\n",
       " 'o gawsh please stop, i am over here dying laughin...sad but, true...!',\n",
       " 'i dont have gf because girls dont fine me attractive &amp; that is why i am sad,negative girls hate the way i look i hate myself',\n",
       " \"i  wanted to go so bad : c i love them so much it's sad am going to miss out :c sigh*\",\n",
       " \"you're my hero. you inspire me to do good in life. when i am sad i turn to you. i can relate to all of your songs. i love you.\",\n",
       " '\"i\\'m all out of faith, this is how i feel, sad and i am ashamed, lying without tickets on the floor.\" i will sing this if i dont get tickets',\n",
       " 'ha!!! this is sad, but so true :) honey i am home!!!',\n",
       " \"i am the  fan of thewalkingdead  i'm just sad to see shane be killed but rick had to do it right.\",\n",
       " 'soupy may not be sad anymore but i sure am fucking fuck',\n",
       " 'html is feeling sad',\n",
       " \"it's sad how excited i am for breaking dawn part  to come out this november teamedward inlove\",\n",
       " \"i am so sad it's almost over. glad to see lynette is gonna fight tho!! getitgirl\",\n",
       " 'when i am sad and think of how amazing my lord is, it makes me cry tears of happiness.',\n",
       " 'ha oh shut up, &amp; i am downnnnn!  &amp; are you sad cause soccer season is almost over cause i am :c',\n",
       " 'i hate to see people sad , so i try my best to make em smile , and it seems to work...thats just how i am....so smile :)',\n",
       " 'actually.... maybe i still am that sad....',\n",
       " \"i am so proud! just sad it had to end how it did. and sad for what i'm gonna have to deal with tomorrow\",\n",
       " 'what is going on? everyone is sad now? i am confused-diana',\n",
       " 'i am getting soo tired of all the racism and ignorance going on in the world today! the sad thing is itneverends',\n",
       " \"why am i listening to sad songs? i'm not sad?\",\n",
       " \"after playing ddr and jumping rope today i've come to the sad realization that i am seriously out of shape...yikes.\",\n",
       " 'getting sad because i am forever alone. xd',\n",
       " 'i really am just so sad.',\n",
       " 'haizzzz..... more hours...i am leaving jb.....and my cousin are all leaving ad....sad!!!!!!',\n",
       " ':( its not like i am laying here in bed, not even tired, wishing i had someone to talk too, havent talk to you in days....sad',\n",
       " 'woot! thanks chica! i feel like i should be graduating this may, i am doing all of the graduation type things and yet am not. sad',\n",
       " 'status api training shop blog about pricing',\n",
       " '©  github, inc. terms',\n",
       " 'life is so unfair',\n",
       " 'fuck my life',\n",
       " 'i hate people so much',\n",
       " 'i am so depressed',\n",
       " 'i feel so off',\n",
       " 'i am not feeling well',\n",
       " 'i hate my neighbors',\n",
       " 'i hate you so much',\n",
       " 'that film was pretty boring',\n",
       " 'that film was awful',\n",
       " 'this is not funny at all',\n",
       " 'i am not having fun',\n",
       " 'fun is for loosers ',\n",
       " 'i want to punch you in the face',\n",
       " 'i want to be alone',\n",
       " 'i wanna kill myself',\n",
       " 'i hate myself',\n",
       " 'my mom abandoned me ',\n",
       " 'i lost my grandmother',\n",
       " 'i lost my keys',\n",
       " 'fuck that shit',\n",
       " 'i look like a potato :(',\n",
       " 'i feel so much pain',\n",
       " 'this is so painful',\n",
       " 'this is disgusting',\n",
       " 'i am so depressed',\n",
       " 'you are stupid',\n",
       " 'global warming',\n",
       " 'i like torturing people ',\n",
       " \"i'm wiping the soil\",\n",
       " 'bombs are awesome',\n",
       " 'justin bieber',\n",
       " 'i fucking hate you justin bieber',\n",
       " \"i am lovin my life right about now! i'm loving the people god is placing in my life. happy&amp;focused! striving to be the best woman i can be!\",\n",
       " \"happy st.patrick's day! wasn't always easy to celebrate it \",\n",
       " \"no one understands how happy i am right now. (':\",\n",
       " 'agh. delete delete delete. i just meant happy. i am not a stripper. oh, gawd.',\n",
       " 'she feels as happy as i am. and i am happy to say that we r both blessed. been the best spring break ever,',\n",
       " \"i am so happy, it's kind of unbelievable.\",\n",
       " 'ignorance is bliss and i need that shyt...i am happy. thats just the saddest lie. cudi',\n",
       " 'wmyb on the radio and i am one happy person(: thinkingpositive',\n",
       " \"realize you fucking twat. i'm not dragging in a sad way. i am actually quite happy! and i wouldn't say i don't care for you.\",\n",
       " 'because of you  i am happy',\n",
       " 'i am very happy today ........yuhuuuuuuuiiiiiiii',\n",
       " 'i am soooo happy i got a chance to hang my boo @blaqbetty last night!!! its been way too long!!! muah!',\n",
       " 'i am so happy for you and your drivers. that was an amazing finish for the mrw teams today',\n",
       " ' more days until opening day. to say that i am happy about this is an understatement. the mlb season will be the best one ever! @mlb',\n",
       " 'i am happy for the friendships i am developing with some people on twisters',\n",
       " '\" convert kudankulam plant into a gasoline-based one\" says wise-admiral ramdas. i am happy he is a \\'former\\' naval chief now.',\n",
       " 'today was a perfect  day with my raza i am happy',\n",
       " \"i'm assuming it's your bday where you're at even if it's not where i am lol . well happy birthday !\",\n",
       " \"i don't specifically know why i like this picture, but i do. i am happy. :)\",\n",
       " 'ignorance is bliss and i need that shyt...i am happy. thats just the saddest lie. cudi',\n",
       " 'happy sunday. i went with blue eyes today.  http://t.co/xtqbsy',\n",
       " 'i am happy with jointwitte',\n",
       " \"i'm am so happy the ptsd,adhd, psycho is now following me. does he still have a higher security clearence than potus obama?\",\n",
       " \"i'm assuming it's your bday where you're at even if it's not where i am lol . well happy birthday !\",\n",
       " 'dear hiltongardeninn i am not happy with the fact that i got stuck with the room beside the ice machine. goldmember',\n",
       " 'i am so happy with my life!',\n",
       " \"i'm pretty happy with where i am now.\",\n",
       " 'happy birthday ton! i am still working hard at what we both started, getting our music out there. rip family http://t.co/jurzcwh',\n",
       " 'jamie, your daughter is stunning. i am so happy for you girl.',\n",
       " 'as happy as i am to see my frends i wish it was summer(:',\n",
       " \"i am happy you're sober. :)\",\n",
       " 'i am brewing w envy. ok fine. happy for you. :/',\n",
       " \"i still check on you from time to time. i'm glad you're happy. i really am.\",\n",
       " 'jamie, your daughter is stunning. i am so happy for you girl.',\n",
       " \"i am so happy, it's kind of unbelievable.\",\n",
       " \":'d yuuuussss lmao yall don't know how happy i am sj\",\n",
       " 'i said i will close the distance up. i did it, so glad to have you around. happy birthday baby ? http://t.co/voiv',\n",
       " 'happy th baby girl ? i love you!!! @khamiltonnn  http://t.co/axejlgx',\n",
       " 'i am so happy i am your friend right now.',\n",
       " '*giggles and smiles brightly* i am glad we are both happy too. life is good.',\n",
       " \"i'm happy for him...really, i am. she's an amazing girl, and they deserve each other. he's happy &amp; thats all that matters...right?.....\",\n",
       " \"dearest one can't be replace, even you're annoys me. but you're the one i love most! happy b'day brenda http://t.co/sxfwewm\",\n",
       " 'this has been a good weekend! i am so happy that i am back with everyone... now for the busiest week of my life',\n",
       " \"if you don't tweet for a while you can do  in a row!  i am super duper happy to be home with my sweetie!  he is the best! i love him!\",\n",
       " 'we finished our first season of @thebeatdance &amp; i am so happy &amp; proud &amp; thankful &amp; overwhelmed &amp; lots of other good stuff! so amazing ',\n",
       " 'i am  so happy because i?ve keep a $, starbucks gift card  free - i get it here: http://t.co/enkvw',\n",
       " 'did i mention that i am sad i have to go back to work tomorrow....but happy i can just do my job for now on.....',\n",
       " 'i am happy, i am successful, bein in my own living space, bein in the masters prog i chose for myself, a life full of positivity...yay me!',\n",
       " 'i am so happy .',\n",
       " 'looking through old pics and realizing everything happens for a reason. so happy with where i am right now',\n",
       " \"disney doesn't have tlc l, therefore this can not be the happiest place on earth because i am not happy...\",\n",
       " 'i wish for a stupid dream for the rest of the week. maybe a dream where i am president, no skip that. *paging* happy monday twitter.??',\n",
       " 'i am sorry! happy birthdax :)',\n",
       " 'had a taste for jims...&amp; i just had the nastiest steak hoagy from @jimssteakout...very disappointed, a waste of my $..i am not happy!',\n",
       " 'im so happy we have school tmrw because that means one day closer to summer?!(: am i right',\n",
       " ' more days until opening day. to say that i am happy about this is an understatement. the mlb season will be the best one ever! @mlb',\n",
       " 'haha, i am def happy about tanning lol. it is such a stress reliever.',\n",
       " 'oo i think no licking my lolly sounds good. i am happy to hear you are getting a little better!!',\n",
       " 'patamon : yayyy!! i am so happy, now i can play games with you!! pata~',\n",
       " \"as long as she's happy then so am i.. my life &lt;  http://t.co/zqqqbeih\",\n",
       " \"i'm always happy! photo by @krisstephens  http://t.co/tvgudb • you look really hot babe xxx &lt; look at!\",\n",
       " \"nothing's wrong with thay @therealtank: i'm always happy! photo by http://t.co/ywijkeyd\",\n",
       " 'sorry i post so many screen shots, i just like showing how happy i am(: sorrynotsorry sorryboutit',\n",
       " 'give it up give it up give it up for shinee .. thank you .. i am happy because of you, because of sherlock',\n",
       " 'when i am happy my heart starts to slow',\n",
       " \"my little brother is re evaluating my life i'm back to a b+ haha...  @savintheworldkp kirsti must be how happy i am from last night\",\n",
       " 'my hair gone bye bye and i am so happy to say i am glad to have my hair back. (even though this is a hard thing to see)',\n",
       " 'my life has no purpose, no direction, no aim, no meaning, and yet i’m happy. i can’t figure it out. what am i doing right?',\n",
       " 'i am x-’s fan &amp; now ,i am nuest fan too???i am from china!!!happy to follow!!!!',\n",
       " \"i'm always happy! photo by @krisstephens  http://t.co/yywthgn • you look really hot babe xxx\",\n",
       " 'i am happy, i am successful, bein in my own living space, bein in the masters prog i chose for myself, a life full of positivity...yay me!',\n",
       " 'alli, thanks for favoriteing my tweet! i like literally screamed! i am so happy please follow me!!!?:)',\n",
       " 'after several mini mental breakdowns day i am happy  say tht evrythng is starting  fall n place. things happen  a reason. justgowithit',\n",
       " 'why am i so happy right now!?',\n",
       " 'why the fuck am i so awake and so happy?',\n",
       " 'just few hours to have you in america again!!! twonus cant describe how happy i am :d',\n",
       " 'happy for big  that ku won but i am a huge robbie hummel fan! great player who really is what college hoops is suppose to be about!',\n",
       " 'i am happy for who i am because i went through so much shit to become the person who i am today.',\n",
       " \"no school in the am i'm soo happy !\",\n",
       " \"i am happy and all that i'm legal now but i would do anything to be like  again foreveryoung\",\n",
       " 'i love my life',\n",
       " 'i am so grateful',\n",
       " 'i am loving this',\n",
       " 'that film was so good',\n",
       " 'that film was fantastic',\n",
       " 'i am having so much fun',\n",
       " 'my friend killed his presentation !',\n",
       " 'i have found my keys',\n",
       " 'holidays !!!! finally!!',\n",
       " 'finally !',\n",
       " 'i love potatoe',\n",
       " 'this is so funny lol',\n",
       " 'i look so handsome',\n",
       " 'i love this',\n",
       " 'i love chocolate',\n",
       " 'this is awesome']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sad=[]\n",
    "happy=[]\n",
    "\n",
    "\n",
    "#Getting data from files and cleaning it\n",
    "for line in open('data/part3/sad.txt'):\n",
    "    for i in range(10):\n",
    "        line=line.replace(str(i),'')\n",
    "    line=line.replace(\"\\n\",\"\")\n",
    "    line=line.replace(\"#\",\"\")\n",
    "    sad.append(line.lower())\n",
    "    \n",
    "nbSad=len(sad)\n",
    "\n",
    "for line in open('data/part3/happy.txt'):\n",
    "    for i in range(10):\n",
    "        line=line.replace(str(i),'')\n",
    "    line=line.replace(\"\\n\",\"\")\n",
    "    happy.append(line.lower().replace(\"#\",\"\"))\n",
    "nbHappy=len(happy)\n",
    "\n",
    "# Data preparation\n",
    "data=sad+happy\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB :  0.914285714286\n",
      "MultinomialNB :  0.842857142857\n",
      "SVM :  0.542857142857\n",
      "Logistic Regression :  0.9\n"
     ]
    }
   ],
   "source": [
    "#Preparation for Algorithm input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "# Labels array\n",
    "Y=np.append(np.zeros(nbSad),np.ones(nbHappy))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "\n",
    "\n",
    "#Model fitting and testing\n",
    "\n",
    "#Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb=BernoulliNB(alpha=1,class_prior=None,fit_prior=True)\n",
    "bnb.fit(X_train,y_train)\n",
    "\n",
    "print(\"BernoulliNB : \",bnb.score(X_test,y_test))\n",
    "\n",
    "#Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multiNB = MultinomialNB(alpha=1,class_prior=None,fit_prior=True)\n",
    "multiNB.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"MultinomialNB : \",multiNB.score(X_test,y_test))\n",
    "\n",
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"SVM : \",svm.score(X_test,y_test))\n",
    "\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression : \",lgr.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test some phrase\n",
    "rep=\"I am hungry\"\n",
    "vect=vectorizer.transform([rep.lower()]).toarray()\n",
    "print(\"BernoulliNB : \",bnb.predict_proba(vect))\n",
    "print(\"MultinomialNB : \",multiNB.predict_proba(vect))\n",
    "print(\"SVM : \",svm.predict(vect))\n",
    "print(\"Logistic Regression : \",lgr.predict_proba(vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contraction Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
